<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="api_pullaudioframe">
    <title><ph keyref="pullAudioFrame"/></title>
    <shortdesc id="short"><ph id="shortdesc">Pulls the remote audio data.</ph></shortdesc>
    <prolog>
        <metadata>
   <keywords>
       <indexterm keyref="pullAudioFrame" />
   </keywords>
        </metadata>
    </prolog>
    <refbody><section id="prototype">
        <p outputclass="codeblock">
                <codeblock props="android" outputclass="language-java">public abstract int pullPlaybackAudioFrame(byte[] data, int lengthInByte);</codeblock>
                <codeblock props="ios mac" outputclass="language-objectivec">- (BOOL)pullPlaybackAudioFrameRawData:(void * _Nonnull)data lengthInByte:(NSUInteger)lengthInByte;</codeblock>
                <codeblock props="windows" outputclass="language-cpp">virtual int pullAudioFrame(IAudioFrameObserver::AudioFrame* frame) = 0;</codeblock>
                <codeblock props="electron" outputclass="language-typescript"/>
                <codeblock props="unity" outputclass="language-csharp"/>
                <codeblock props="rn" outputclass="language-typescript"/>
                <codeblock props="flutter" outputclass="language-dart"/>
        </p>
        </section>
        <section id="detailed_desc">
            <p props="rtc">使用该方法前，你需要调用 <xref keyref="setExternalAudioSink"/> 通知 app 开启并设置外部渲染。</p>
            <p props="rtc-ng"><ph props="android windows">使用该方法前，你需要在 <xref keyref="RtcEngineConfig"/> 中设置 <parmname props="android">mEnableAudioDevice</parmname><parmname props="windows">enableAudioDevice</parmname> 为 <codeph><ph keyref="false"/></codeph>， 并调用 <xref keyref="setExternalAudioSink"/> 通知 app 开启并设置外部渲染。 </ph> <ph props="ios mac">使用该方法前，你需要调用 <xref keyref="setExternalAudioSink"/> 通知 app 开启并设置外部渲染。</ph></p>
   <p>After a successful method call, the app pulls the decoded and mixed audio data for playback.</p>
   <note type="attention">
       <ul>
           <li>Once you enable the external audio sink, the app will not<xref keyref="onPlaybackAudioFrame"/> retrieve any audio data from the callback.</li>
           <li>The difference between this method and the<apiname keyref="onPlaybackAudioFrame"/> callback is as follows:<ul><li>: The SDK sends the audio data to the app through this<apiname keyref="onPlaybackAudioFrame"/> callback. Any delay in processing the audio frames may result in audio jitter.</li>
                   <li>调用该方法后 app 会主动拉取音频数据。 After setting the audio data parameters, the SDK adjusts the frame buffer and avoids problems caused by jitter in the external audio playback.</li>
               </ul></li>
       </ul>
   </note>
        </section>
        <section id="parameters"><title>Parameter</title>
   <parml>
       <plentry props="windows">
  <pt>frame</pt>
  <pd>Pointers to<xref keyref="AudioFrame"/> .</pd>
       </plentry>
       <plentry props="android ios mac">
           <pt>data</pt>
           <pd>待拉取的远端音频数据，数据类型为 <codeph>byte[]</codeph>。</pd>
       </plentry>
       <plentry props="android ios mac" id="length">
           <pt>lengthInByte</pt>
           <pd>The data length (byte). 该参数的值由音频数据时长、<apiname keyref="setExternalAudioSink"/> 的 <codeph>sampleRate</codeph> 和 <codeph>channels</codeph> 参数确定。 <codeph> lengthInByte</codeph> = <codeph>sampleRate</codeph>/1000 × 2 × <codeph>channels</codeph> × audio data duration (ms)</pd>
       </plentry>
   </parml>
        </section>
        <section id="return_values">
   <title>Returns</title>
   <ul props="android windows">
       <li>Success.</li>
       <li>&lt; 0: Failure.</li>
   </ul>
            <ul props="ios mac">
       <li><codeph>Success.</codeph><ph keyref="true"/></li>
       <li><codeph>&lt;Failure.</codeph><ph keyref="false"/></li>
   </ul> 
        </section></refbody>
</reference>
