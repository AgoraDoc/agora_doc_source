<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="class_audioparams">
    <title><ph keyref="AudioParams" /></title>
    <shortdesc id="short"><ph id="shortdesc">Audio data format.</ph></shortdesc>
    <refbody>
        <section id="prototype">
            <p outputclass="codeblock">
            <codeblock props="android" outputclass="language-java">int mute(boolean mute);</codeblock>
            <codeblock props="ios mac" outputclass="language-objectivec">- (int)mute:(bool)isMute;</codeblock>
            <codeblock props="cpp" outputclass="language-cpp">virtual int mute(bool mute) = 0;</codeblock>
            <codeblock props="electron" outputclass="language-typescript">abstract mute(muted: boolean): number;</codeblock>
            <codeblock props="unity" outputclass="language-csharp">public abstract int Mute(bool muted);</codeblock>
            <codeblock props="rn" outputclass="language-typescript">abstract mute(muted: boolean): number;</codeblock>
            <codeblock props="flutter" outputclass="language-dart">Future&lt;void&gt; mute(bool muted);</codeblock>
            <codeblock props="unreal" outputclass="language-cpp" /></p>
        </section>
        <section id="detailed_desc">
            <p props="native unity">You can pass the <apiname keyref="AudioParams" /> object in the return value of the following callbacksto set the audio data format for the corresponding callback:<ul>
            <li><xref keyref="getRecordAudioParams" />: Sets the audio data format for the <xref keyref="onRecordAudioFrame" /> callback.</li>
            <li><xref keyref="getPlaybackAudioParams" />: Sets the audio data format for the <xref keyref="onPlaybackAudioFrame" /> callback.</li>
            <li><xref keyref="getMixedAudioParams" />: Sets the audio data format for the <xref keyref="onMixedAudioFrame" /> callback.</li>
            <li><xref keyref="getEarMonitoringAudioParams" />: Sets the audio data format for the <xref keyref="onEarMonitoringAudioFrame" /> callback.</li>    
            </ul></p>
            <p props="electron rn">The SDK sets the audio data format in the following callbacks according to <apiname keyref="AudioParams" />.<ul>
            <li><xref keyref="onRecordAudioFrame" /> </li>
            <li><xref keyref="onPlaybackAudioFrame" /> </li>
            <li><xref keyref="onMixedAudioFrame" /> </li>
            </ul></p>            
            <note type="attention">
            <ul>
            <li>The SDK calculates the sampling interval through the <parmname>samplesPerCall</parmname>, <parmname>sampleRate</parmname>, and <parmname>channel</parmname> parameters in <apiname keyref="AudioParams" />, and triggers the <apiname keyref="onRecordAudioFrame" />, <apiname keyref="onPlaybackAudioFrame" />, <apiname keyref="onMixedAudioFrame" />, and <apiname keyref="onEarMonitoringAudioFrame" /> callbacks according to the sampling interval.</li>
            <li><equation-inline><parmname>Sample</parmname> interval (<parmname>sec</parmname>) = <parmname>samplePerCall</parmname>/(<parmname>sampleRate</parmname> × <parmname>channel</parmname>)</equation-inline>.</li>
            <li>Ensure that the sample interval ≥ 0.01 (s).</li>
            </ul></note> </section>
        <section id="parameters">
            <title><text conref="../conref/conref_api_metadata.dita#conref_api_metadata/property" /></title>
            <parml>
            <plentry>
                <pt props="android ios mac flutter">sampleRate</pt>
                <pt props="cpp unity electron rn">sample_rate</pt>
                <pd>The audio sample rate (Hz), which can be set as one of the following values:<ul>
                    <li>8000.</li>
                    <li>(Default) 16000.</li>
                    <li>32000.</li>
                    <li>44100</li>
                    <li>48000</li>
                    </ul></pd>
            </plentry>
            <plentry>
                <pt props="android ios mac">channel</pt>
                <pt props="cpp unity electron rn flutter">channels</pt>
                <pd>The number of audio channels, which can be set as either of the following values:<ul>
                    <li>1: (Default) Mono.</li>
                    <li>2: Stereo.</li>
                    </ul></pd>
            </plentry>
            <plentry>
                <pt>mode</pt>
                <pd props="ios mac cpp unity electron rn flutter">The use mode of the audio data. See <xref keyref="RAW_AUDIO_FRAME_OP_MODE_TYPE" />.</pd>
                <pd props="android">The use mode of the audio data, which can be set as either of the following values:<ul>
                    <li><ph keyref="RAW_AUDIO_FRAME_OP_MODE_READ_ONLY" />(0): Read-only mode, <ph props="cpp">where users only read the original data from <xref keyref="AudioFrame" /> without any modification. </ph>For example, when users acquire the data with the Agora SDK, then start the media push.</li>
                    <li><ph keyref="RAW_AUDIO_FRAME_OP_MODE_READ_WRITE" />(2): Read and write mode, <ph props="cpp">where users read the data from <xref keyref="AudioFrame" />, modify it, and then play it. </ph><ph props="electron">Users read the data returned by the SDK, modify it, and then play it. </ph>For example, when users have their own audio-effect processing module and perform some voice pre-processing, such as a voice change.</li>
                    </ul> </pd>
            </plentry>
            <plentry>
                <pt props="android ios mac flutter">samplesPerCall</pt>
                <pt props="cpp unity electron rn">samples_per_call</pt>
                <pd>The number of samples, such as 1024 for the media push.</pd>
            </plentry>
            </parml> </section>
    </refbody>
</reference>